# Ollama Chat
Chat with your local LLM model inside your terminal!

## Features
- Your and LLM's messages are stored in an array so it is like a real conversation.
- Results are streaming so you dont have to wait for all of the response.

## Installation
- Make sure you already installed ollama inside your computer.
  - After installing it, please serve your preferred LLM model with `ollama run model_name`.
- Clone the repo.
- `npm install` to install all dependencies.
- Edit `model` key at line:16 accordingly to your model preferrence.
- `node index.js` to start the converstaion.
