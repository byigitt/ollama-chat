# ollama-chat
chat with your local LLM model inside your terminal
